# Theories of Deep Learning course

This is a GitHub page of Theories of Deep Learning course held by Neural Networks and Deep Learning Lab., MIPT. The working language of this course is Russian.

Lecture slides and homework assignments will be placed in this repo.

Location: Moscow Institute of Physics and Technology, Phystech.Bio building, room *TBA*

Time: Friday, 10:45, starting from 15th of February, 2019.

Videos will be available.

Further announcements will be in our Telegram chat: https://t.me/joinchat/D_ljjxJHIrD8IuFvfqVLPw

## Syllabus:

This syllabus is not final and may change. The order of topics will change with high probability.

1. Introduction.

2. Statistical learning theory.
    * Formalization of learning task. PAC-learning. Empirical risk. Generalization guarantees. Finite and infinite hypothesis classes. VC-dimension. PAC-Bayes framework.

3. Generalization guarantees.
    * Covering numbers, McDiarmidâ€™s inequality. Application of PAC-Bayesian framework. Compression framework. Examples of computing generalization guarantees. Link between generalization and loss surface properties.

4. Loss surfaces of neural networks.
    * Loss surface of linear networks. Loss surfaces of deep and shallow non-linear networks. Spin-glass model. Elimination of local minima.

5. Gradient descent dynamics.
    * GD almost surely does not converge to strict saddles. Convergence guarantees for noisy GD. GD dynamics on linear networks. GD dynamics on wide shallow non-linear networks. Generalization to deep nets.

6. Information propagation.
    * Necessary conditions of learning. Neural networks from the view of random matrix theory.

7. The information bottleneck method.
    * Definition. Learning phases. Critics.

## Course staff:

- [Eugene Golikov](https://github.com/varenick)
- [Maksim Kretov](https://github.com/kretovmk)
