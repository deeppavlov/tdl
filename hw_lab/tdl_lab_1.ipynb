{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: paths between minima of neural nets.\n",
    "\n",
    "### 10 points total + 2 extra points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Disclaimer: we've tested the lab, however, if something doesn't work, you are free to change the given template.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan:\n",
    "\n",
    "In this lab we are going to do some quantitative research concerning the loss landscape of neural nets. Specifically, we will try to answer the following two questions:\n",
    "1. How \"chaotic\" is the loss landscape? Specifically, how common are local minima and saddle points?\n",
    "2. Are local minima found by stochastic gradient descent (SGD) isolated or are they connected with a valley of small loss?\n",
    "\n",
    "First, following [Goodfellow et al. (2014)](https://arxiv.org/abs/1412.6544), we are going to look at the loss on the linear segment connecting two different points:\n",
    "1. Two random points;\n",
    "2. Minimum found by SGD and its initialization point;\n",
    "3. Minimum found by SGD and some other initialization point;\n",
    "4. Two different minima found by SGD.\n",
    "\n",
    "Then, following [Garipov et al. (2018)](https://arxiv.org/abs/1802.10026), we are going to fit the valley of small loss between two minima found by SGD (if it exists) with a simple curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Tuple\n",
    "from time import time\n",
    "from copy import copy, deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# We use pytorch 1.0 in our labs (we didn't test on other versions); \n",
    "# installation instructions are here: https://pytorch.org/ \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.distributions as distr\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "input_shape = [1, 28, 28]\n",
    "output_size = 10 # number of classes in MNIST\n",
    "batch_size = 128 # you can change this number\n",
    "\n",
    "dataloader_kwargs = {'num_workers': 0, 'pin_memory': True} if USE_CUDA else {}\n",
    "\n",
    "dataset_train = MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "dataset_test = MNIST('./data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train, batch_size=batch_size, shuffle=True, **dataloader_kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=batch_size, shuffle=True, **dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReshapeLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "A layer that accepts a tensor of shape [batch_size, *valid_shape]\\\n",
    "and returns a tensor of shape [batch_size, *target_shape],\\\n",
    "where valid_shape is compatible with target_shape.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, *target_shape):\n",
    "        super(ReshapeLayer, self).__init__()\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(x.shape[0], *self.target_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will experiment with a simple multi-layer fully-connected network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClassifierFC(nn.Module):\n",
    "    \"\"\"\n",
    "    \n",
    "A fully-connected network with 'num_hidden' hidden layers of 'hidden_dim' neurons each,\\\n",
    "alternated with instances of 'nonlinearity'.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, output_size, hidden_dim=50, num_hidden=1, nonlinearity=nn.ReLU):\n",
    "        super(type(self), self).__init__()\n",
    "        assert num_hidden >= 1, \"the case 'hum_hidden' = 0 is not implemented\"\n",
    "        input_size = int(np.prod(input_shape))\n",
    "        self.layers = nn.ModuleList([\n",
    "            ReshapeLayer(input_size),\n",
    "            nn.Linear(input_size, hidden_dim),\n",
    "            nonlinearity()\n",
    "        ])\n",
    "        for _ in range(num_hidden-1):\n",
    "            self.layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.layers.append(nonlinearity())\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_size))\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "\n",
    "Forward pass through the network. Returns logits.\n",
    "Expected input shape: [batch_size, *self.input_shape]\n",
    "Output shape: [batch_size, output_size].\n",
    "\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NullContext:\n",
    "    def __enter__(self):\n",
    "        pass\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our main train/validation routine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_epoch(model, loader, loss_function, optimizer=None):\n",
    "    \"\"\"\n",
    "\n",
    "Performs one training or testing epoch, returns a tuple of mean loss and mean accuracy.\\\n",
    "If 'optimizer' is not None, performs an optimization step.\n",
    "\n",
    "    \"\"\"\n",
    "    is_train = optimizer is not None\n",
    "    \n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    cum_loss = 0\n",
    "    cum_acc = 0\n",
    "    cum_batch_size = 0\n",
    "\n",
    "    with NullContext() if is_train else torch.no_grad():\n",
    "        for X, y in loader:\n",
    "            batch_size = X.shape[0]\n",
    "            cum_batch_size += batch_size\n",
    "\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits = model(X)\n",
    "            loss = loss_function(logits, y)\n",
    "            cum_loss += loss.item() * batch_size\n",
    "\n",
    "            acc = torch.mean((torch.max(logits, dim=-1)[1] == y).float())\n",
    "            cum_acc += acc.item() * batch_size\n",
    "\n",
    "            if is_train:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "    mean_loss = cum_loss / cum_batch_size\n",
    "    mean_acc = cum_acc / cum_batch_size\n",
    "\n",
    "    return mean_loss, mean_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Straight path between minima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_minima = 2\n",
    "\n",
    "state_dicts_at_minima = []\n",
    "state_dicts_at_inits = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1 (1 point)\n",
    "Define your network. Train the network twice. Try to achieve 100% **train** accuracy (you'll probably have to decay your learning rate in some way). Save weights at initialization to *state_dicts_at_inits* and trained weights to *state_dicts_at_minima*.\n",
    "\n",
    "*Remark: why do we need 100% train accuracy? Empirically, the default network (1 hidden layer with 64 neurons and ReLU nonlinearity) always achieves 100% train accuracy when properly trained (with decaying learning rate etc.), and optimization becomes easier when the network grows larger (but not too large). So, if you didn't achieve it, you are either in a \"bad\" local minimum, or you haven't trained your network hard enough (or, simply, you have a bug). According to our observations, the first alternative is quite unlikely for the default network. Hence being below 100% train accuracy mean being far from minimum.*\n",
    "\n",
    "*Remark2. If you didn't achieve 100% train accuracy, and you are sure that you actually converged to a local minimum, please, save the result, weights of the network, network specification, random seed, etc; everything to reproduce this; and let me know.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100 # you can change this number\n",
    "\n",
    "for minimum_idx in range(num_minima):\n",
    "    model = ClassifierFC(input_shape, output_size, num_hidden=1, hidden_dim=64, nonlinearity=nn.ReLU).to(device)\n",
    "    \n",
    "    # The model state (its weights, in particular) is stored in model.state_dict();\n",
    "    # you can print it somewhere, if you are curious.\n",
    "    # Here we append our freshly-initialized model state to state_dicts_at_inits:\n",
    "    state_dicts_at_inits.append(deepcopy(model.state_dict()))\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    # YOUR CODE: create an optimizer and a learning rate scheduler (pytorch has everything you need: see torch.optim)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('epoch', epoch+1)\n",
    "        \n",
    "        # YOUR CODE: optimize your network for one epoch (use perform_epoch)\n",
    "\n",
    "    # Here we append our trained model state to state_dicts_at_minima:\n",
    "    state_dicts_at_minima.append(deepcopy(model.state_dict()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2 (2 points)\n",
    "Compute train and test loss and accuracy on a straight line connecting two of the previously stored states. Plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpolate_between_state_dicts(t, state_dict_a, state_dict_b):\n",
    "    state_dict = OrderedDict()\n",
    "    # YOUR CODE: compute a linear combination of two state dicts with coefficients t and 1-t\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_losses = {}\n",
    "train_accs = {}\n",
    "test_losses = {}\n",
    "test_accs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It is interesting to slightly extend the segment between two minima:\n",
    "ts = np.linspace(0, 1.5, num=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'linear_combination_of_two_minima'\n",
    "\n",
    "train_losses[mode] = []\n",
    "train_accs[mode] = []\n",
    "\n",
    "test_losses[mode] = []\n",
    "test_accs[mode] = []\n",
    "\n",
    "for t in tqdm_notebook(ts):\n",
    "    # YOUR CODE: compute train and test loss and accuracy \n",
    "    # for a linear combination of model weights at two previously found minima \n",
    "    # with coefficients t and 1-t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode = 'linear_combination_of_minimum_and_init'\n",
    "\n",
    "# YOUR CODE: do the same, but interpolate between one of the minima and the corresponding weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode = 'linear_combination_of_minimum_and_random'\n",
    "\n",
    "# YOUR CODE: the same, but interpolate between one of the minima and initialization for another minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode = 'linear_combination_of_two_randoms'\n",
    "\n",
    "# YOUR CODE: the same, but interpolate between two pre-saved weight initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the corresponding results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: make nice plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Charactarize loss and accuracy on segments you've just tried."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions to think about:\n",
    "* Can you characterize the loss landscape as \"chaotic\" or \"regular\"?\n",
    "* Do train and test curves look quantitatively same or differently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Constructing an optimal one-bend chain connecting two minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have two points $w_1$ and $w_2$ which are both minima of a loss of our network on the train dataset $\\mathcal{L}_{train}(w)$. \n",
    "Let $\\theta$ be some point in the weight-space.\n",
    "Let $\\phi_{\\theta}(t)$ be a chain composed of two linear segments: first one connecting $w_1$ and $\\theta$, and the second one connecting $\\theta$ and $w_2$. \n",
    "We choose parameterization such that $\\phi_{\\theta}(0) = w_1$, $\\phi_{\\theta}(1) = w_2$ and $\\phi_{\\theta}(0.5) = \\theta$.\n",
    "\n",
    "Recall we are looking for a valley of small loss.\n",
    "We are going to approximate this valley with our one-bend chain.\n",
    "In order to do this we are going to minimize an average loss over the chain:\n",
    "$$\n",
    "\\mathcal{L}_{chain}(\\theta) = \\mathbb{E}_{w \\sim U(\\phi_{\\theta})} \\mathcal{L}_{train}(w) \\to \\min_{\\theta},\n",
    "$$\n",
    "where $U(\\phi_{\\theta})$ denotes a uniform distribution of weights over chain.\n",
    "\n",
    "It is not easy to optimize this objective with SGD. That's why we further follow [Garipov et al. (2018)](https://arxiv.org/abs/1802.10026) to approximate our objective as follows:\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}_{chain}(\\theta) = \\mathbb{E}_{t \\sim U[0,1]} \\mathcal{L}_{train}(\\phi_\\theta(t)) \\to \\min_{\\theta}.\n",
    "$$\n",
    "\n",
    "The latter objective could be easily optimized with gradient methods:\n",
    "$$\n",
    "\\nabla_{\\theta} \\tilde{\\mathcal{L}}_{chain}(\\theta) = \\mathbb{E}_{t \\sim U[0,1]} \\nabla_{\\theta} \\mathcal{L}_{train}(\\phi_\\theta(t)),\n",
    "$$\n",
    "where we can unbiasedly estimate the expectation over $t$ with Monte-Carlo samples:\n",
    "$$\n",
    "\\mathbb{E}_{t \\sim U[0,1]} \\nabla_{\\theta} \\mathcal{L}_{train}(\\phi_\\theta(t)) \\approx\n",
    "\\nabla_{\\theta} \\mathcal{L}_{train}(\\phi_\\theta(\\tilde t)), \\quad \\tilde{t} \\sim U[0,1].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 (1 point)\n",
    "Derive an exact formula for $\\phi_{\\theta}(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 (2 points)\n",
    "Find an optimal bend for two minima found before. Where you able to optimize a bend to small mean loss on a chain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we initialize state_dict_at_theta as a state dict \n",
    "# in the middle of the segment connecting two minima we have previously found;\n",
    "# theta_params is a list of trainable model weights at theta; these are the weights we are going to optimize:\n",
    "state_dict_at_theta = OrderedDict()\n",
    "theta_params = []\n",
    "for param_key in state_dicts_at_minima[0].keys():\n",
    "    state_dict_at_theta[param_key] = (state_dicts_at_minima[0][param_key] + state_dicts_at_minima[1][param_key]) / 2\n",
    "    try:\n",
    "        state_dict_at_theta[param_key].requires_grad_()\n",
    "        theta_params.append(state_dict_at_theta[param_key])\n",
    "    except RuntimeError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_state_dict_from_chain(t, state_dict_at_w1, state_dict_at_w2, state_dict_at_theta):\n",
    "    state_dict = OrderedDict()\n",
    "    # YOUR CODE: compute a state dict at point t of the bend following formula you have just derived\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE: create an optimizer and a learning rate scheduler for theta_params we are going to optimize\n",
    "num_epochs = 100 # you are free to change this number\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('epoch', epoch+1)\n",
    "\n",
    "    cum_loss = 0\n",
    "    cum_acc = 0\n",
    "    cum_batch_size = 0\n",
    "\n",
    "    model.eval()\n",
    "    for X, y in train_loader:\n",
    "        batch_size = X.shape[0]\n",
    "        cum_batch_size += batch_size\n",
    "\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        t = np.random.rand()\n",
    "        # YOUR CODE: set state_dict of a model to a point on a chain that corresponds to t\n",
    "        # HINT: use model.load_state_dict()\n",
    "\n",
    "        logits = model(X)\n",
    "        loss = loss_function(logits, y)\n",
    "        cum_loss += loss.item() * batch_size\n",
    "\n",
    "        acc = torch.mean((torch.max(logits, dim=-1)[1] == y).float())\n",
    "        cum_acc += acc.item() * batch_size\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # YOUR CODE: first, compute gradients of train loss wrt model weights at \\phi_{\\theta}(t);\n",
    "        # then, given them, compute gradients of train loss at \\phi_{\\theta}(t) wrt \\theta\n",
    "        optimizer.step()\n",
    "\n",
    "    mean_train_loss = cum_loss / cum_batch_size\n",
    "    mean_train_acc = cum_acc / cum_batch_size\n",
    "\n",
    "    print('train:')\n",
    "    print('loss = {:.4f}; acc = {:.2f}'.format(mean_train_loss, mean_train_acc*100))\n",
    "\n",
    "    # Uncomment if you use lr scheduler:\n",
    "    #scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize train and test loss and accuracy on a chain you have found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = '1-bend_chain'\n",
    "\n",
    "train_losses[mode] = []\n",
    "train_accs[mode] = []\n",
    "\n",
    "test_losses[mode] = []\n",
    "test_accs[mode] = []\n",
    "\n",
    "for t in tqdm_notebook(ts):\n",
    "    # TODO: compute train and test loss and accuracy at the chain you've just found\n",
    "    # for a given t using formula you've derived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: more nice plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare losses / accuracy on a bend and on a segment connecting two minima:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE: even more nice plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 (1 point):\n",
    "Ok, we've just found an optimal bend $\\theta$ for two different minima $w_1$ and $w_2$.\n",
    "Define $u = w_2 - w_1$ and $v = \\theta - w_1$.\n",
    "As long as $\\theta$ doesn't lie on a straight line connecting $w_1$ and $w_2$, our chain lie on a plane spanned by vectors $u$ and $v$.\n",
    "\n",
    "However, in order to visualize loss on this plane nicely, we need to derive an orthogonal basis there.\n",
    "Vectors $u$ and $v$ are not generally orthogonal.\n",
    "\n",
    "First, derive two vectors $\\hat u$ and $\\hat v$ that span the same plane as $u$ and $v$ while being orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u_state_dict = OrderedDict()\n",
    "v_state_dict = OrderedDict()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for param_key in state_dict_at_theta.keys():\n",
    "        u_state_dict[param_key] = state_dicts_at_minima[1][param_key] - state_dicts_at_minima[0][param_key]\n",
    "        v_state_dict[param_key] = state_dict_at_theta[param_key] - state_dicts_at_minima[0][param_key]\n",
    "\n",
    "hat_u_state_dict = OrderedDict()\n",
    "hat_v_state_dict = OrderedDict()\n",
    "\n",
    "# YOUR CODE: compute hat_u_state_dict and hat_v_state_dict using formula you've just derived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4 (1 point)\n",
    "Visualize your network train loss on a plane spanned by $\\hat u$ and $\\hat v$; visualize your chain on the same plot. Do you see a valley of small loss? Does your chain approximate this valley well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_losses_grid = []\n",
    "train_accs_grid = []\n",
    "\n",
    "test_losses_grid = []\n",
    "test_accs_grid = []\n",
    "\n",
    "ts = np.linspace(-1, 2, num=30) # this specification heavily depends on lenghts of vectors \\hat u and \\hat v;\n",
    "ss = np.linspace(-1, 2, num=30) # hence you probably will need to modify it\n",
    "\n",
    "for t in tqdm_notebook(ts):\n",
    "    train_losses_grid.append([])\n",
    "    train_accs_grid.append([])\n",
    "    \n",
    "    test_losses_grid.append([])\n",
    "    test_accs_grid.append([])\n",
    "    \n",
    "    for s in tqdm_notebook(ss):\n",
    "        state_dict = OrderedDict()\n",
    "        # YOUR CODE: compute state_dict on a plane spanned by hat_u_state_dict and hat_v_state_dict with coeffs t and s;\n",
    "        # load it to the model; compute train and test loss and accuracy at this point\n",
    "        # HINT: evaluating model on a whole train/test dataset can take too much time;\n",
    "        # if iterations run to slowly, you can evaluate your model on some fixed subset of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE: make contour plots\n",
    "# Attention: there could be very high loss at the plane you span;\n",
    "# so, in order to see the actual valley, you will probably have to adjust 'levels' argument of plt.contourf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 (2 points) \n",
    "Try deeper/wider network. Try sigmoid instead of ReLU. Does anything change quantitatively? If yes, indicate what.\n",
    "You can just clone this notebook and do the same with new network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 3.2 (optional; 2 points extra) \n",
    "Take a serious convolutional architecture with ~~black-jack~~ batch-norms and dropouts, say, VGG (see, https://pytorch.org/docs/stable/torchvision/models.html#id2). Perform the same set of experiments on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
